{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L0PWpLd57OkE",
      "metadata": {
        "id": "L0PWpLd57OkE"
      },
      "outputs": [],
      "source": [
        "! pip install optuna textstat category_encoders\n",
        "! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WHhLq4Ye8qXu",
      "metadata": {
        "id": "WHhLq4Ye8qXu"
      },
      "outputs": [],
      "source": [
        "! unzip Data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70b65a7e-54aa-414c-974a-02b4aacb0869",
      "metadata": {
        "id": "70b65a7e-54aa-414c-974a-02b4aacb0869"
      },
      "outputs": [],
      "source": [
        "import os, random, optuna, textstat\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "from scipy.stats import mode\n",
        "from sklearn.model_selection import cross_validate, KFold, RepeatedKFold, train_test_split\n",
        "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder, RobustScaler, OrdinalEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, balanced_accuracy_score\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.neighbors import NearestNeighbors, LocalOutlierFactor\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.base import TransformerMixin\n",
        "\n",
        "import category_encoders as ce\n",
        "\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor, early_stopping, Dataset\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "plt.rc(\"figure\", autolayout=True)\n",
        "plt.rc(\n",
        "    \"axes\",\n",
        "    labelweight=\"bold\",\n",
        "    labelsize=\"large\",\n",
        "    titleweight=\"bold\",\n",
        "    titlesize=14,\n",
        "    titlepad=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown \"1d0XpolF_YMrXX_Wib6lMZVQMgBt2bpc0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58Pxxp1Bmu8M",
        "outputId": "78a72e50-34fc-4a7f-d65c-20f7dcc0101f"
      },
      "id": "58Pxxp1Bmu8M",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1d0XpolF_YMrXX_Wib6lMZVQMgBt2bpc0\n",
            "To: /content/power.csv\n",
            "\r  0% 0.00/14.2M [00:00<?, ?B/s]\r100% 14.2M/14.2M [00:00<00:00, 204MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5955ac5c-58bf-4dfa-88be-24b9678af071",
      "metadata": {
        "id": "5955ac5c-58bf-4dfa-88be-24b9678af071"
      },
      "outputs": [],
      "source": [
        "'''Set dataset directory'''\n",
        "ROOT_DIR = \"/content/Data\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef781d9b-5ae3-479e-82a1-466ef7749457",
      "metadata": {
        "id": "ef781d9b-5ae3-479e-82a1-466ef7749457"
      },
      "source": [
        "# 1. Exploratory Data Analysis\n",
        "- look at the dataset basics (size of the data, data types, look at a few examples etc.)\n",
        "- look for any missing data\n",
        "- look at target value\n",
        "- look for any outliers in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72605f49-bc1f-4971-96ae-3d32f6ca1ab1",
      "metadata": {
        "id": "72605f49-bc1f-4971-96ae-3d32f6ca1ab1"
      },
      "source": [
        "## 1(a) Profile the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa302a1-7ccc-48ce-8f3f-0257f0d1474c",
      "metadata": {
        "id": "5fa302a1-7ccc-48ce-8f3f-0257f0d1474c"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(ROOT_DIR, \"power.csv\"))\n",
        "\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e18458b4-9d3a-4a13-9316-62988203b35b",
      "metadata": {
        "id": "e18458b4-9d3a-4a13-9316-62988203b35b"
      },
      "outputs": [],
      "source": [
        "print(\"df shape:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xedfRci88-9O",
      "metadata": {
        "id": "xedfRci88-9O"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f96c5dbc-e0b7-4cd2-beb0-19dbd97c000a",
      "metadata": {
        "id": "f96c5dbc-e0b7-4cd2-beb0-19dbd97c000a"
      },
      "outputs": [],
      "source": [
        "ProfileReport(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebbaa82f-aa21-4efe-9141-33a03d7a4be9",
      "metadata": {
        "id": "ebbaa82f-aa21-4efe-9141-33a03d7a4be9"
      },
      "source": [
        "From the initial dataset profiling:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8938819a-3feb-45b3-8909-75b70f1c7490",
      "metadata": {
        "id": "8938819a-3feb-45b3-8909-75b70f1c7490"
      },
      "source": [
        "## 1(b) look at the missing values\n",
        "- look for any patterns in missing data\n",
        "- look at some examples of missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1ddb092-f251-4d4e-88fa-70512384f001",
      "metadata": {
        "id": "d1ddb092-f251-4d4e-88fa-70512384f001"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fdbc8fa-537c-4923-85b8-a93427697c84",
      "metadata": {
        "id": "1fdbc8fa-537c-4923-85b8-a93427697c84"
      },
      "source": [
        "## 1(c) Look at the target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ac499ab-5242-4716-9a3f-39933a3c2282",
      "metadata": {
        "id": "1ac499ab-5242-4716-9a3f-39933a3c2282"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.distplot(df[''])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f8f6605-a255-471c-a897-774f867054cb",
      "metadata": {
        "id": "6f8f6605-a255-471c-a897-774f867054cb"
      },
      "source": [
        "## 1(d) Look for Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sdy6bX2q_oHp",
      "metadata": {
        "id": "Sdy6bX2q_oHp"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sJN9lcuUA3qL",
      "metadata": {
        "id": "sJN9lcuUA3qL"
      },
      "outputs": [],
      "source": [
        "variable =''\n",
        "\n",
        "df[(df[variable] > df[variable].mean()+3*df[variable].std()) | (df[variable] < df[variable].mean()-3*df[variable].std())]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8339136c-42ae-45fc-bd16-cae097d8c49d",
      "metadata": {
        "id": "8339136c-42ae-45fc-bd16-cae097d8c49d",
        "tags": []
      },
      "source": [
        "# 2. Import and Preprocess Data\n",
        "- import some helper functions to do imputation and deal with data outliers\n",
        "- use a function to do the data import and cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "870635a6-ee2c-4d40-918d-b756048676af",
      "metadata": {
        "id": "870635a6-ee2c-4d40-918d-b756048676af"
      },
      "outputs": [],
      "source": [
        "def simple_impute(df):\n",
        "    '''\n",
        "    Impute the numerical columns by the median value for each column and\n",
        "    impute the categorical columns by the most frequent, or mode, for each column\n",
        "    Note: one can easily switch in different imputers for each of the data types to something like kNN or iterative\n",
        "    '''\n",
        "    df= df.copy()\n",
        "    # Impute missing values for numerical data\n",
        "    # imp_num = IterativeImputer(estimator=ExtraTreesRegressor(), initial_strategy='median', max_iter=20)\n",
        "    imp_num = SimpleImputer(strategy='median')\n",
        "    numerical_df = df.select_dtypes(\"number\")\n",
        "    numerical_df = pd.DataFrame(data=imp_num.fit_transform(numerical_df), index=df.index, columns =numerical_df.columns)\n",
        "    \n",
        "    if df.select_dtypes(\"category\").shape[1] >0:\n",
        "        # Imput missing values for categorical data\n",
        "        # imp_cat = IterativeImputer(estimator=ExtraTreesClassifier(), initial_strategy='most_frequent', max_iter=20)\n",
        "        imp_cat = SimpleImputer(strategy='most_frequent')\n",
        "        categorical_df = df.select_dtypes(\"category\")\n",
        "        enc = OrdinalEncoder()\n",
        "        categorical_df = pd.DataFrame(data=enc.fit_transform(categorical_df), columns=categorical_df.columns)\n",
        "        categorical_imputations = enc.inverse_transform(imp_cat.fit_transform(categorical_df))\n",
        "        categorical_df = pd.DataFrame(data=categorical_imputations, index=df.index, columns =categorical_df.columns, dtype=\"category\")\n",
        "        return categorical_df.join(numerical_df).reindex(columns= df.columns)\n",
        "    else:\n",
        "        return numerical_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8547868-cf34-4b06-9523-63bfec5b8364",
      "metadata": {
        "id": "e8547868-cf34-4b06-9523-63bfec5b8364"
      },
      "outputs": [],
      "source": [
        "class ML_Impute(TransformerMixin):\n",
        "      '''\n",
        "      Impute missing values by treating the imputational as a machine learning problem. For numerical\n",
        "      columns, we can treat the problem as a regression problem, and for categorical, a classification problem.\n",
        "      For this method, we'll iterate through all of the columns with one column being the target variable\n",
        "      and the others as being predictor variables\n",
        "      '''\n",
        "\n",
        "    def __init__(self, params={}):\n",
        "        self.params = {}\n",
        "        self.models = {}\n",
        "\n",
        "    def fit(self, df):\n",
        "        df = df.copy()\n",
        "        # label encode categorical variables\n",
        "        columns = df.columns.to_list()\n",
        "        cat_cols = df.select_dtypes(\"category\").columns.to_list()\n",
        "        self.enc = OrdinalEncoder()\n",
        "        df[cat_cols] = self.enc.fit_transform(df[cat_cols])\n",
        "\n",
        "        # Randomized column selection\n",
        "        for i in random.sample(range(len(df.columns)), len(df.columns)):\n",
        "\n",
        "        # Starting with most null values to least\n",
        "        # for i in np.argsort(-df.isnull().sum().values):\n",
        "            column = columns[i]\n",
        "            # Check to make sure there are null values that need to be imputed\n",
        "            if not df[column].isnull().any():\n",
        "                continue\n",
        "\n",
        "            print(\"Creating Imputation Model for Column: {}\".format(column))\n",
        "\n",
        "            # Create train, test, and validation data using the null values of the column of interest\n",
        "            X= df.loc[df[column].notnull()]\n",
        "            y = X.pop(column)\n",
        "            X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "            X_test = df.loc[df[column].isnull()]\n",
        "            _ = X_test.pop(column)\n",
        "\n",
        "            # If we have more data, we use more estimators for the imputation model\n",
        "            n_estimators = min(100, int(len(X_train) / 10))\n",
        "            if column in cat_cols:\n",
        "                model = LGBMClassifier(**self.params, verbose=-1, n_estimators=n_estimators)\n",
        "            else:\n",
        "                model = LGBMRegressor(**self.params, verbose=-1, n_estimators=n_estimators)\n",
        "\n",
        "            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],\n",
        "                      callbacks = [early_stopping(min(20, int(len(X_train) / 10)))])\n",
        "            print(\"Score of Column {} is {}\".format(column, model.score(X,y)))\n",
        "            self.models[column] = model \n",
        "\n",
        "    def transform(self, df):\n",
        "        df = df.copy()\n",
        "\n",
        "        # label encode categorical variables\n",
        "        columns = df.columns.to_list()\n",
        "        cat_cols = df.select_dtypes(\"category\").columns.to_list()\n",
        "        df[cat_cols] = self.enc.transform(df[cat_cols])\n",
        "\n",
        "        for column in self.models.keys():\n",
        "          X_test = df.loc[df[column].isnull()]\n",
        "          _ = X_test.pop(column)\n",
        "          model = self.models[column]\n",
        "          preds =model.predict(X_test)\n",
        "          m = df[column].isna()\n",
        "          df.loc[m, column]  = preds.flatten()\n",
        "\n",
        "        if len(cat_cols) >0:\n",
        "          df[cat_cols] = self.enc.inverse_transform(df[cat_cols])\n",
        "\n",
        "        return df\n",
        "\n",
        "    def fit_transform(self, df):\n",
        "        self.fit(df)\n",
        "        df = self.transform(df)\n",
        "        return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6JZWFbEFB9Lo",
      "metadata": {
        "id": "6JZWFbEFB9Lo"
      },
      "outputs": [],
      "source": [
        "def cap_outliers(df, variables=[None]):\n",
        "    df = df.copy()\n",
        "    for variable in variables:\n",
        "    upper_limit = df[variable].mean() + 3*df[variable].std()\n",
        "    lower_limit = df[variable].mean() - 3*df[variable].std()\n",
        "    df[variable] = np.where(df[variable]> upper_limit, upper_limit, np.where(\n",
        "        df[variable]<lower_limit, lower_limit, df[variable]\n",
        "    ))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75ef91dd-cda0-4298-b493-0156e091ceb3",
      "metadata": {
        "id": "75ef91dd-cda0-4298-b493-0156e091ceb3"
      },
      "outputs": [],
      "source": [
        "# Wrapper function to read in, encode and impute missing values for the data\n",
        "\n",
        "def preprocess_data(df, cat_features=[None], outlier_features=[None]):\n",
        "    \n",
        "    # Specify categorical variables\n",
        "    for name in cat_features:\n",
        "        df[name] = df[name].astype(\"category\")\n",
        "        # Add a None category for missing values\n",
        "        if \"None\" not in df[name].cat.categories:\n",
        "            df[name].cat.add_categories(\"None\", inplace=True)\n",
        "\n",
        "    # create test data set\n",
        "    train, test = train_test_split(df, test_size=0.2)\n",
        "\n",
        "    # Preprocessing\n",
        "    imputer = ML_Impute()\n",
        "    train = imputer.fit_transform(train)\n",
        "    train = cap_outliers(train, outlier_features)\n",
        "\n",
        "    test = imputer.transform(test)\n",
        "    test = cap_outliers(test, outlier_features)\n",
        "\n",
        "\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be2ee427-5728-4bfe-95ca-7e977fb891be",
      "metadata": {
        "id": "be2ee427-5728-4bfe-95ca-7e977fb891be"
      },
      "outputs": [],
      "source": [
        "#Now, load in and preprocess the data\n",
        "\n",
        "df = pd.read_csv(os.path.join(ROOT_DIR, \"\"), index_col='')\n",
        "target_var = \"\"\n",
        "\n",
        "train, test = load_data(df, cat_features = [], outlier_features=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc8ec75-2575-441f-bf28-5ff9664fc39b",
      "metadata": {
        "id": "2dc8ec75-2575-441f-bf28-5ff9664fc39b"
      },
      "source": [
        "# 3. Create a Baseline\n",
        "- specify a baseline scoring function\n",
        "- create a baseline model. \n",
        "  - For regression use `LGBMRegressor` and `scoring='neg_root_mean_squared_error'` and multiply the result by `-1`\n",
        "  - For classification use `LGBMClassifier` and `scoring='balanced_accuracy'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65ecf5d3-860b-456a-b86a-678de670630e",
      "metadata": {
        "id": "65ecf5d3-860b-456a-b86a-678de670630e"
      },
      "outputs": [],
      "source": [
        "def score_dataset(X, y, \n",
        "                  model=LGBMRegressor(n_estimators=1000, verbose=-1, random_state=42)\n",
        "                 ):\n",
        "    \n",
        "    \n",
        "    scores = cross_validate(\n",
        "        model, X, y, cv=5, n_jobs=-1, scoring='neg_root_mean_squared_error', return_train_score=True\n",
        "    )\n",
        "    \n",
        "    return {\"Training\":-1*np.mean(scores[\"train_score\"]), \"Validation\":-1*np.mean(scores[\"test_score\"])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdb0b75e-9d33-4927-9ea8-eb3118f04d34",
      "metadata": {
        "id": "fdb0b75e-9d33-4927-9ea8-eb3118f04d34"
      },
      "outputs": [],
      "source": [
        "X = train.copy()\n",
        "y = X.pop(target_var)\n",
        "X = ce.OrdinalEncoder().fit_transform(X)\n",
        "\n",
        "score_dataset(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e295babf-8d18-443b-abb7-fea6222be667",
      "metadata": {
        "id": "e295babf-8d18-443b-abb7-fea6222be667",
        "tags": []
      },
      "source": [
        "# 4. Featurize the Data\n",
        "- remove uniformative features\n",
        "- create interactions\n",
        "- Binning\n",
        "- Indicate Outliers\n",
        "- Try different encodings for categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37e9fb69-5bf6-4a1a-aed2-d6208c584516",
      "metadata": {
        "id": "37e9fb69-5bf6-4a1a-aed2-d6208c584516"
      },
      "outputs": [],
      "source": [
        "def make_mi_scores(X, y):\n",
        "    X = X.copy()\n",
        "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
        "        X[colname], _ = X[colname].factorize()\n",
        "    # All discrete features should now have integer dtypes\n",
        "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
        "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
        "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
        "    mi_scores = mi_scores.sort_values(ascending=False)\n",
        "    return mi_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_bErwNVFH_a5",
      "metadata": {
        "id": "_bErwNVFH_a5"
      },
      "outputs": [],
      "source": [
        "X = train.copy()\n",
        "y = X.pop(target_var)\n",
        "mi_scores = make_mi_scores(X, y)\n",
        "mi_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af24a6b-0233-42b1-953e-e142b5cf8a6c",
      "metadata": {
        "id": "9af24a6b-0233-42b1-953e-e142b5cf8a6c"
      },
      "outputs": [],
      "source": [
        "# Try removing some of the uninformative features to see if that improves scores\n",
        "uninformative_features = [\n",
        "\n",
        "]\n",
        "\n",
        "X = train.copy()\n",
        "y = X.pop(target_var)\n",
        "X = X.loc[:,~X.columns.isin(uninformative_features)]\n",
        "X = ce.OrdinalEncoder().fit_transform(X)\n",
        "\n",
        "score_dataset(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EPCRuCtzJ7uS",
      "metadata": {
        "id": "EPCRuCtzJ7uS"
      },
      "source": [
        "For mathematical transforms, try things like log transforms:\n",
        "\n",
        "`X['feature'] = np.log1p(X['feature'])`\n",
        "\n",
        "For interations try multiplying or dividing features, especially between levels\n",
        "of a categorical feature and a continuous feature. Use subject matter expertise here\n",
        "\n",
        "`df_new['A_B_interation'] = ce.OneHotEncoder().fit_transform(X['A']) * X['B']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QaC0WBgFJ5Nf",
      "metadata": {
        "id": "QaC0WBgFJ5Nf"
      },
      "outputs": [],
      "source": [
        "def mathematical_transforms(df):\n",
        "    X = pd.DataFrame(index=df.index)\n",
        "\n",
        "    return X\n",
        "\n",
        "def interactions(df):\n",
        "    X = pd.DataFrame(index=df.index)\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vl6bFWROKs-3",
      "metadata": {
        "id": "vl6bFWROKs-3"
      },
      "outputs": [],
      "source": [
        "X = train.copy()\n",
        "y = X.pop(target_var)\n",
        "#X = X.join(mathematical_transforms(X))\n",
        "#X = X.join(interactions(X))\n",
        "X = ce.OrdinalEncoder().fit_transform(X)\n",
        "\n",
        "score_dataset(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ipv16eN3K5j0",
      "metadata": {
        "id": "Ipv16eN3K5j0"
      },
      "source": [
        "Notes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C_S9gZfYIiZ4",
      "metadata": {
        "id": "C_S9gZfYIiZ4"
      },
      "outputs": [],
      "source": [
        "def cluster_labels(df, features, n_clusters=10):\n",
        "    X = df.copy()\n",
        "    X_scaled = X.loc[:, features]\n",
        "    X_scaled = ce.OneHotEncoder().fit_transform(X_scaled)\n",
        "    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / (X_scaled.std(axis=0)+0.000001)\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=50)\n",
        "    X_new = pd.DataFrame(index=X.index)\n",
        "    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
        "    X_new[\"Cluster\"] = X_new[\"Cluster\"].astype(\"category\")\n",
        "    return X_new[\"Cluster\"]\n",
        "\n",
        "\n",
        "def cluster_distance(df, features, n_clusters=10):\n",
        "    X = df.copy()\n",
        "    X_scaled = X.loc[:, features]\n",
        "    X_scaled = ce.OneHotEncoder().fit_transform(X_scaled)\n",
        "    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / (X_scaled.std(axis=0)+0.000001)\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=50)\n",
        "    X_cd = kmeans.fit_transform(X_scaled)\n",
        "    # Label features and join to dataset\n",
        "    X_cd = pd.DataFrame(\n",
        "        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])], index=X.index\n",
        "    )\n",
        "    return X_cd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1xBut4HcIqCM",
      "metadata": {
        "id": "1xBut4HcIqCM"
      },
      "outputs": [],
      "source": [
        "cluster_features = [\n",
        "    \n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wRTQdmVcIqjG",
      "metadata": {
        "id": "wRTQdmVcIqjG"
      },
      "outputs": [],
      "source": [
        "X = train.copy()\n",
        "y = X.pop(target_var)\n",
        "X = X.join(cluster_distance(X, vape, n_clusters=10))\n",
        "X = ce.OrdinalEncoder().fit_transform(X)\n",
        "\n",
        "score_dataset(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jTiUmSfUJKvB",
      "metadata": {
        "id": "jTiUmSfUJKvB"
      },
      "outputs": [],
      "source": [
        "X = train.copy()\n",
        "y = X.pop(target_var)\n",
        "X = X.join(cluster_labels(X, vape, n_clusters=10))\n",
        "X = ce.OrdinalEncoder().fit_transform(X)\n",
        "\n",
        "score_dataset(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l6jPeNQpLIPO",
      "metadata": {
        "id": "l6jPeNQpLIPO"
      },
      "source": [
        "Notes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uJPD5PVzLJpX",
      "metadata": {
        "id": "uJPD5PVzLJpX"
      },
      "outputs": [],
      "source": [
        "def flag_outliers(df):\n",
        "    df = df.copy()\n",
        "    df = ce.OneHotEncoder().fit_transform(df)\n",
        "    clf = LocalOutlierFactor()\n",
        "    df[\"Outlier\"] = clf.fit_predict(df)\n",
        "    return df[\"Outlier\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yorht94RM1X9",
      "metadata": {
        "id": "yorht94RM1X9"
      },
      "outputs": [],
      "source": [
        "X = train.copy()\n",
        "y = X.pop(target_var)\n",
        "X = X.join(flag_outliers(X))\n",
        "X = ce.OrdinalEncoder().fit_transform(X)\n",
        "\n",
        "score_dataset(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JdR7Quw_NRF9",
      "metadata": {
        "id": "JdR7Quw_NRF9"
      },
      "source": [
        "Notes:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78600898-c660-4b0f-99a9-39b7a773ad5e",
      "metadata": {
        "id": "78600898-c660-4b0f-99a9-39b7a773ad5e"
      },
      "source": [
        "# 5. Finalize Features for Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6adc8836-a8ea-4cfa-96f1-0ea73e59328d",
      "metadata": {
        "id": "6adc8836-a8ea-4cfa-96f1-0ea73e59328d"
      },
      "outputs": [],
      "source": [
        "def create_features(df, df_test=None):\n",
        "    X = df.copy()\n",
        "    \n",
        "    if df_test is not None:\n",
        "        X_test = df_test.copy()\n",
        "        X = pd.concat([X, X_test])\n",
        "        \n",
        "    # Add in engineered features\n",
        "    X = X.join(flag_outliers(X))\n",
        "    X = ce.OrdinalEncoder().fit_transform(X)\n",
        "\n",
        "    \n",
        "    # Reform splits\n",
        "    if df_test is not None:\n",
        "        X_test = X.loc[df_test.index, :]\n",
        "        X.drop(df_test.index, inplace=True)\n",
        "    \n",
        "\n",
        "    if df_test is not None:\n",
        "        return X, X_test\n",
        "    else:\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "575e5ae4-2ba8-4a7f-bd39-e580e5a1df76",
      "metadata": {
        "id": "575e5ae4-2ba8-4a7f-bd39-e580e5a1df76"
      },
      "outputs": [],
      "source": [
        "X = train.copy()\n",
        "X_test = test.copy()\n",
        "y = X.pop(target_var)\n",
        "y_test = X_test.pop(target_var)\n",
        "\n",
        "X, X_test = create_features(train, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e923da-06ff-4996-ace2-e2d034ba2cef",
      "metadata": {
        "id": "48e923da-06ff-4996-ace2-e2d034ba2cef"
      },
      "outputs": [],
      "source": [
        "score_dataset(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f418a52e-5c10-48a9-a658-e17f1d586255",
      "metadata": {
        "id": "f418a52e-5c10-48a9-a658-e17f1d586255"
      },
      "source": [
        "# 6. Hyperparameter Tuning\n",
        "- you need to specify the objective (i.e. `'regression'`, `'multiclass'` with '`num_class`', etc.) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d85c9cfc-ad6e-452e-87a5-630404590170",
      "metadata": {
        "id": "d85c9cfc-ad6e-452e-87a5-630404590170"
      },
      "outputs": [],
      "source": [
        "def objective(trial, X, y):\n",
        "    # Specify a search space using distributions across plausible values of hyperparameters.\n",
        "    param = {\n",
        "        \"objective\": \"\",\n",
        "        \"verbosity\": -1,              \n",
        "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
        "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n",
        "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n",
        "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 0, 15),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n",
        "    }\n",
        "    \n",
        "    # Run LightGBM for the hyperparameter values\n",
        "    lgbcv = lgb.cv(param,\n",
        "                   lgb.Dataset(X, label=y),\n",
        "                   folds= KFold(n_splits=5, shuffle=True),\n",
        "                   verbose_eval=False,                   \n",
        "                   early_stopping_rounds=10,                   \n",
        "                   num_boost_round=100\n",
        "                  )\n",
        "    \n",
        "    cv_score = lgbcv['l2-mean'][-1]\n",
        "    \n",
        "    # Return metric of interest\n",
        "    return cv_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cfb70ba-c8b4-4a7f-97fe-2f8d6c3989c3",
      "metadata": {
        "id": "2cfb70ba-c8b4-4a7f-97fe-2f8d6c3989c3",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(lambda trial: objective(trial, X, y), timeout=300, n_trials=5) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "865f4d54-39d8-4cbe-8b83-90600413a2e3",
      "metadata": {
        "id": "865f4d54-39d8-4cbe-8b83-90600413a2e3"
      },
      "outputs": [],
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ba1849e-46ba-4666-9cad-a76fb239622d",
      "metadata": {
        "id": "9ba1849e-46ba-4666-9cad-a76fb239622d"
      },
      "outputs": [],
      "source": [
        "print(study.best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de91670-0e83-4b19-af9f-8f9d9c5ae6f3",
      "metadata": {
        "id": "3de91670-0e83-4b19-af9f-8f9d9c5ae6f3"
      },
      "outputs": [],
      "source": [
        "print(study.best_value**0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f1512f9-1d87-450e-a379-f8c6abe291f4",
      "metadata": {
        "id": "9f1512f9-1d87-450e-a379-f8c6abe291f4"
      },
      "source": [
        "# 7. Fit final model and check predictions\n",
        "- you need to specify the objective (i.e. `'regression'`, `'multiclass'` with '`num_class`', etc.) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8a30b2-de9d-44e0-baee-7154b28ee6e5",
      "metadata": {
        "id": "9f8a30b2-de9d-44e0-baee-7154b28ee6e5",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "preds = []\n",
        "skf = RepeatedKFold(n_splits=3, n_repeats=2)\n",
        "\n",
        "for fold_idx, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
        "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "    lgb_train = lgb.Dataset(X_train, y_train)\n",
        "    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
        "    lgb_params = {\n",
        "        'objective': '',\n",
        "        'verbose': -1,\n",
        "        'n_estimators': 500,\n",
        "        **study.best_params\n",
        "    }\n",
        "    model = lgb.train(lgb_params, lgb_train, valid_sets=lgb_eval, callbacks=[lgb.early_stopping(10)])\n",
        "\n",
        "    y_pred = model.predict(X_valid)\n",
        "    score = mean_squared_error(y_valid, y_pred, squared=False)\n",
        "    print(\"Fold {} MSE Score: {}\".format(fold_idx, score))\n",
        "    print(\"----------------------\")\n",
        "    preds.append( model.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09bbcd7d-340f-45d0-8478-b220f65e97f1",
      "metadata": {
        "id": "09bbcd7d-340f-45d0-8478-b220f65e97f1"
      },
      "outputs": [],
      "source": [
        "# Use average for ensembling of the labels\n",
        "\n",
        "final_preds = np.mean(preds, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30a7dbd3-c6d8-45d4-a943-f3125b57212e",
      "metadata": {
        "id": "30a7dbd3-c6d8-45d4-a943-f3125b57212e"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, sharey=True, figsize=(16,8))\n",
        "sns.distplot(y_test, ax=axs[0])\n",
        "axs[0].set_title(\"Distribution of Test Target Variable\")\n",
        "sns.distplot(final_preds , ax=axs[1])\n",
        "axs[1].set_title(\"Distribution of Predicted Target Variable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66c9e33d-1eca-4c9a-b6c0-42a79a5a8ccf",
      "metadata": {
        "id": "66c9e33d-1eca-4c9a-b6c0-42a79a5a8ccf"
      },
      "outputs": [],
      "source": [
        "print(\"Test Accuracy: {}\".format(mean_squared_error(y_test, final_preds, squared=False)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yjDNx8pIQyJf",
      "metadata": {
        "id": "yjDNx8pIQyJf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}