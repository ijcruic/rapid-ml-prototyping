{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label YouTube Comments for their Stance toward the U.S. Army\n",
    "\n",
    "For this tutorial we are going to create labels for the [stance](https://www.sciencedirect.com/science/article/pii/S0306457322001728) of comments toward videos on the U.S. Army's official [YouTube Channel](https://www.youtube.com/USarmy). This type of labeling task is common for things like public affairs, political science, or marketing where we want yto get metrics on how certain messages are being received by the (a) public. \n",
    "\n",
    "In this context stance is defined as the opinion, either expressed or implied, of a user or text toward a target. Typically, stance is either labeled as 'for', 'against', 'neutral', and can include 'unrelated'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for labeling data by LLM\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and inspect the dataset to be labeled\n",
    "\n",
    "We will read in the validation dataset, which has human annotations to compare to, for this exercise. The full data set is available [here](https://zenodo.org/records/10493803)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"@usarmy_comments_validation_set_labels.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get an LLM working\n",
    "\n",
    "For this exercise, we will stand up a local (relatively) small LLM, in this case a [specially tuned T5 model](https://huggingface.co/declare-lab/flan-alpaca-gpt4-xl). It should be noted that if you want to use a decoder-only model (i.e., Llama, Mistral, etc.) you need to switch to a `text-generation` pipeline. Also, setting `return_full_text=False` when using a text-generation pipeline is also helpful as it just returns what the model generates and not the full prompt.\n",
    "\n",
    "Once we have the pipeline, we wrap it in langchain's pipeline class so that we can use it in chains.\n",
    "\n",
    "Finally, one can also use a closed-source model, like OpenAI as well. Just consult [the documentation](https://python.langchain.com/docs/integrations/chat/openai/) to see how to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model using Hugging Face pipeline\n",
    "hf_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"declare-lab/flan-alpaca-gpt4-xl\",\n",
    "    device=-1,  # Use CPU (-1 for CPU, other numbers for GPUs)\n",
    "    max_new_tokens = 100,\n",
    ")\n",
    "\n",
    "# Create the LangChain LLM using the HuggingFace pipeline\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run some examples \n",
    "question = '''Analyze the following social media post and determine its stance towards the provided entity. Respond with a single word: \"for\", \"against\", \"neutral\", or \"unrelated\". Only return the stance as a single word, and no other text.\n",
    "entity: U.S. Army\n",
    "post: @vondeveen If the Army wants to actually recruit people, maybe stop breaking people and actually prosecute sexual assualt #nomorewar.\n",
    "stance:'''\n",
    "print(llm.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''Analyze the following social media post and determine its stance towards the provided entity. Respond with a single word: \"for\", \"against\", \"neutral\", or \"unrelated\". Only return the stance as a single word, and no other text.\n",
    "entity: U.S. Army\n",
    "post: @artfulask I have never seen a pink-eared duck before. #Army\n",
    "stance:'''\n",
    "print(llm.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''Analyze the following social media post and determine its stance towards the provided entity. Respond with a single word: \"for\", \"against\", \"neutral\", or \"unrelated\". Only return the stance as a single word, and no other text.\n",
    "entity: U.S. Army\n",
    "post: I think the @Army helped me become disciplined. I would have surely flunked out of college chasing tail if I didn't get some discipline there. #SFL\n",
    "stance:'''\n",
    "print(llm.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_template = '''Analyze the following YouTube comment to a video posted by the U.S. Army named \"{title}\" and determine its stance towards the provided entity. Respond with a single word: \"for\", \"against\", \"neutral\", or \"unrelated\". Only return the stance as a single word, and no other text.\n",
    "        entity: {entity}    \n",
    "        comment: {statement}    \n",
    "        stance:'''  \n",
    "\n",
    "# Initialize a PromptTemplate object  \n",
    "context_prompt = PromptTemplate(input_variables=[\"title\",\"entity\",\"statement\"], template=context_template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df.iloc[0,:]\n",
    "\n",
    "formated_prompt = context_prompt.format(title=example['name'], \n",
    "                      entity = \"the U.S. Army\",\n",
    "                      statement = example['comment'])\n",
    "\n",
    "print(formated_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Run a Labeling Chain\n",
    "\n",
    "In the newer versions of LangChain, you string together 'runnbales' using the pipe (|) format to create chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = context_prompt | llm |  StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.invoke({\"title\":example['name'], \n",
    "                  \"entity\":\"the U.S. Army\",\n",
    "                  \"statement\":example['comment']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we can programmatically produce labels!\n",
    "\n",
    "results = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Classifying rows\"):\n",
    "    result = llm_chain.invoke({\n",
    "        \"title\": row['name'],\n",
    "        \"entity\": \"the U.S. Army\",\n",
    "        \"statement\": row['comment']\n",
    "    })\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(results, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the output, sometimes we get extra text that we did not ask the LLM for. So, often we want a post-processing function to make sure everythign maps back to the labels we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_results(result):\n",
    "    \"\"\"\n",
    "    This function post-processes the result from a large language model to label text.\n",
    "\n",
    "    Args:\n",
    "        result (str): A string representing the LLM output word.\n",
    "\n",
    "    Returns:\n",
    "        str: A classification label ('disagree', 'neutral', 'agree', or 'unrelated').\n",
    "    \"\"\"\n",
    "    \n",
    "    # Words or phrases that indicate each stance category\n",
    "    disagree_indicators = ['against', 'denies', 'critical', 'deny', 'neg', 'oppose', 'opposes']\n",
    "    agree_indicators = ['support', 'supports', 'for', 'pro ', 'positive', 'agree', 'agrees']\n",
    "    neutral_indicators = ['neutral']\n",
    "\n",
    "    # Normalize the word to lower case and remove leading/trailing white spaces\n",
    "    normalized_word = str(result).strip().lower()\n",
    "\n",
    "    # Determine stance based on the indicators\n",
    "    if any(indicator in normalized_word for indicator in disagree_indicators):\n",
    "        # If the word is also found in agree_indicators or neutral_indicators, label it as 'neutral'\n",
    "        if any(indicator in normalized_word for indicator in agree_indicators) or any(indicator in normalized_word for indicator in neutral_indicators):\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return 'against'\n",
    "    elif any(indicator in normalized_word for indicator in neutral_indicators):\n",
    "        # If the word is also found in disagree_indicators or agree_indicators, label it as 'neutral'\n",
    "        if any(indicator in normalized_word for indicator in disagree_indicators) or any(indicator in normalized_word for indicator in agree_indicators):\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    elif any(indicator in normalized_word for indicator in agree_indicators):\n",
    "        # If the word is also found in disagree_indicators or neutral_indicators, label it as 'neutral'\n",
    "        if any(indicator in normalized_word for indicator in disagree_indicators) or any(indicator in normalized_word for indicator in neutral_indicators):\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return 'for'\n",
    "    else:\n",
    "        # If no specific stance label is found, label it as unrelated\n",
    "        return 'unrelated'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [post_process_results(i) for i in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(results, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering for Labeling Data by LLM\n",
    "\n",
    "Okay, having seen how we can classify the stance of the comments toward a target (in this case, the U.S. Army), lets look at how we can construct some other labeling prompts, based on some of the design patterns we talked about earlier. Specifically, lets look at:\n",
    "- few-shot prompting\n",
    "- chain-of-thought-prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot prompting\n",
    "\n",
    "key to making this work well is the examples you give the LLM to reason on for classifying the stance. these examples coule be drawn from the same dataset, a related dataset or even completely made up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_template = '''title: {title}\n",
    "entity: {entity}\n",
    "comment: {comment}\n",
    "stance: {stance}'''\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"title\", \"entity\", \"comment\", \"stance\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {'title': \"New Recruitment Video\",\n",
    "     'entity': \"the U.S. Army\",\n",
    "     'comment': \"This is an amazing initiative by the Army.\",\n",
    "     'stance': 'for'},\n",
    "    \n",
    "    {'title': \"Training Highlights\",\n",
    "     'entity': \"the U.S. Army\",\n",
    "     'comment': \"This video shows the Army's commitment to readiness.\",\n",
    "     'stance': 'for'},\n",
    "    \n",
    "    {'title': \"Military Expenditure Analysis\",\n",
    "     'entity': \"the U.S. Army\",\n",
    "     'comment': \"Why is so much taxpayer money wasted on this?\",\n",
    "     'stance': 'against'},\n",
    "    \n",
    "    {'title': \"Veterans' Day Tribute\",\n",
    "     'entity': \"the U.S. Army\",\n",
    "     'comment': \"This is a neutral tribute, nothing special.\",\n",
    "     'stance': 'neutral'},\n",
    "    \n",
    "    {'title': \"New Recruitment Video\",\n",
    "     'entity': \"the U.S. Army\",\n",
    "     'comment': \"This has nothing to do with the Army, totally irrelevant.\",\n",
    "     'stance': 'unrelated'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '''Stance classification is the task of determining the stance of a comment towards a specific entity. The following examples illustrate different stances a comment can take: \"for\", \"against\", \"neutral\", or \"unrelated\".'''\n",
    "\n",
    "suffix = '''Analyze the following YouTube comment to a video posted by the U.S. Army named \"{title}\" and determine its stance towards the provided entity. Respond with a single word: \"for\", \"against\", \"neutral\", or \"unrelated\". Only return the stance as a single word, and no other text.\n",
    "title: {title}\n",
    "entity: {entity}\n",
    "comment: {comment}\n",
    "stance:'''\n",
    "\n",
    "# Create the FewShotPromptTemplate using the updated prefix, suffix, and examples\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"title\", \"entity\", \"comment\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_prompt = few_shot_prompt.format(title=example['name'], \n",
    "                      entity = \"the U.S. Army\",\n",
    "                      comment = example['comment'])\n",
    "\n",
    "print(formated_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can simiarly define a chain for the few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_chain = few_shot_prompt | llm |  StrOutputParser() | RunnableLambda(post_process_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_chain.invoke({\"title\":example['name'], \n",
    "                  \"entity\":\"the U.S. Army\",\n",
    "                  \"comment\":example['comment']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-thought prompting\n",
    "\n",
    "This method often requires constructing together multiple prompts, which breakdown and reason over the example to be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT template 1: reason about potential stances\n",
    "\n",
    "cot_template_1 = '''Analyze the following YouTube comment to a video named \"{title}\" posted by the U.S. Army. Consider the opinion, or stance, expressed in the comment about the provided entity. Provide reasoning for your analysis.\n",
    "title: {title}\n",
    "entity: {entity}\n",
    "comment: {comment}\n",
    "explanation:'''\n",
    "\n",
    "cot_prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"title\", \"entity\", \"comment\"],\n",
    "    template=cot_template_1\n",
    ")\n",
    "\n",
    "cot_chain_1 = cot_prompt_1 | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_chain_1.invoke({\"title\":example['name'], \n",
    "                  \"entity\":\"the U.S. Army\",\n",
    "                  \"comment\":example['comment']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT template 1: prodcue the final stance judgement\n",
    "\n",
    "cot_template_2 = '''Based on your explanation, \"{stance_reason}\", what is the final stance towards the provided entity? Respond with a single word: \"for\", \"against\", \"neutral\", or \"unrelated\". Only return the stance as a single word, and no other text.\n",
    "title: {title}\n",
    "entity: {entity}\n",
    "comment: {comment}\n",
    "stance:'''\n",
    "\n",
    "cot_prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"title\", \"entity\", \"comment\", \"stance_reason\"],\n",
    "    template=cot_template_2\n",
    ")\n",
    "\n",
    "cot_chain_2 = cot_prompt_2 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the chains together for labeling data points\n",
    "\n",
    "cot_chain = {\n",
    "    \"stance_reason\": cot_chain_1,\n",
    "    \"title\": RunnablePassthrough(),\n",
    "    \"entity\": RunnablePassthrough(),\n",
    "    \"comment\": RunnablePassthrough()\n",
    "} | cot_chain_2 | StrOutputParser() | RunnableLambda(post_process_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_chain.invoke({\"title\":example['name'], \n",
    "                  \"entity\":\"the U.S. Army\",\n",
    "                  \"comment\":example['comment']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
