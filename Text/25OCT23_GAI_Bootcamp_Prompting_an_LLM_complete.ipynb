{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b37c7bb-c037-4504-a2e2-4010b3726ac8",
   "metadata": {},
   "source": [
    "<div align=\"center\">  \n",
    "  \n",
    "# **Prompt Engineering**  \n",
    "  \n",
    "</div>  \n",
    "  \n",
    "Welcome to this instructional section! We'll delve into the common prompt engineering patterns for the stance classification task. This tutorial is designed to be a step-by-step guide through the process. Let's look at the roadmap for this journey:  \n",
    "  \n",
    "---  \n",
    "  \n",
    "**Table of Contents**  \n",
    "  \n",
    "1. **Environment Configuration**    \n",
    "    Setting up the appropriate environment, including the installation of necessary packages.  \n",
    "  \n",
    "2. **Data Import and Preprocessing**    \n",
    "    Loading the data and transforming it into a suitable format for our tasks.  \n",
    "  \n",
    "3. **Setting Up an LLM to Prompt**    \n",
    "    Establishing a Language Model to initiate the prompting process.  \n",
    "  \n",
    "4. **Exploring Model Outputs**    \n",
    "    Investigating some example outputs from our model.  \n",
    "  \n",
    "5. **Using Langchain for Programmatic Prompting**    \n",
    "    Discovering how to leverage Langchain for easier, more systematic prompting.  \n",
    "  \n",
    "6. **Understanding Fundamental Prompt Engineering Patterns**    \n",
    "    Delving into some of the fundamental patterns of prompt engineering.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "Let's get started!  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace383e1-d872-478a-b36a-703e7c531a0e",
   "metadata": {},
   "source": [
    "# 1. Configure the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b436af4-75ff-4598-a9e3-70e746e1f99d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/jovyan/.local/lib/python3.10/site-packages (0.0.158)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/jovyan/.local/lib/python3.10/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /home/jovyan/.local/lib/python3.10/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /home/jovyan/.local/lib/python3.10/site-packages (from langchain) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/jovyan/.local/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.66.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/jovyan/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/jovyan/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /home/jovyan/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.3->langchain) (3.0.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.9.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Package installations to work on WIRE\n",
    "\n",
    "! pip install transformers\n",
    "! pip install langchain\n",
    "! pip install accelerate\n",
    "! pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e77263-062c-44bc-8ace-6cf3a638e014",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 13:31:52.487701: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-25 13:31:52.487769: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-25 13:31:52.487808: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-25 13:31:52.498353: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-25 13:31:53.292422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, re, pandas as pd, numpy as np, string\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from langchain import PromptTemplate, FewShotPromptTemplate, HuggingFacePipeline, LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import SequentialChain, ConversationChain\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import accelerate\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5057d3-9986-42e2-91f9-ccb0a8a782e5",
   "metadata": {},
   "source": [
    "# 2. Import and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab8789ed-ab80-429d-a03e-6c07ac91a9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = os.path.join(\"/home/jovyan/wire/WIREUsers/icruickshank/LLM-Stance-Labeling/phemerumours/data_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdaa0228-553d-4169-a510-a8a974955d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b749ee9-8fce-4238-bfd9-e4b845d444aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>stance</th>\n",
       "      <th>event</th>\n",
       "      <th>full_text</th>\n",
       "      <th>context</th>\n",
       "      <th>train_stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576755174531862529</td>\n",
       "      <td>agree</td>\n",
       "      <td>Russian President Putin has gone missing</td>\n",
       "      <td>Coup? RT @jimgeraghty: Rumors all Russian mili...</td>\n",
       "      <td>The following statement is a social media post...</td>\n",
       "      <td>supports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>576319832800555008</td>\n",
       "      <td>agree</td>\n",
       "      <td>Russian President Putin has gone missing</td>\n",
       "      <td>Hoppla! @L0gg0l: Swiss Rumors: Putin absence d...</td>\n",
       "      <td>The following statement is a social media post...</td>\n",
       "      <td>supports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>576513463738109954</td>\n",
       "      <td>disagree</td>\n",
       "      <td>Russian President Putin has gone missing</td>\n",
       "      <td>Putin reappears on TV amid claims he is unwell...</td>\n",
       "      <td>The following statement is a social media post...</td>\n",
       "      <td>denies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>552783667052167168</td>\n",
       "      <td>agree</td>\n",
       "      <td>there was a shooting event at Charlie Hebdo in...</td>\n",
       "      <td>France: 10 people dead after shooting at HQ of...</td>\n",
       "      <td>The following statement is a social media post...</td>\n",
       "      <td>supports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>552793679082311680</td>\n",
       "      <td>agree</td>\n",
       "      <td>there was a shooting event at Charlie Hebdo in...</td>\n",
       "      <td>11 confirmed dead, Francois Hollande to visit ...</td>\n",
       "      <td>The following statement is a social media post...</td>\n",
       "      <td>supports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id    stance  \\\n",
       "0  576755174531862529     agree   \n",
       "1  576319832800555008     agree   \n",
       "2  576513463738109954  disagree   \n",
       "3  552783667052167168     agree   \n",
       "4  552793679082311680     agree   \n",
       "\n",
       "                                               event  \\\n",
       "0           Russian President Putin has gone missing   \n",
       "1           Russian President Putin has gone missing   \n",
       "2           Russian President Putin has gone missing   \n",
       "3  there was a shooting event at Charlie Hebdo in...   \n",
       "4  there was a shooting event at Charlie Hebdo in...   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  Coup? RT @jimgeraghty: Rumors all Russian mili...   \n",
       "1  Hoppla! @L0gg0l: Swiss Rumors: Putin absence d...   \n",
       "2  Putin reappears on TV amid claims he is unwell...   \n",
       "3  France: 10 people dead after shooting at HQ of...   \n",
       "4  11 confirmed dead, Francois Hollande to visit ...   \n",
       "\n",
       "                                             context train_stance  \n",
       "0  The following statement is a social media post...     supports  \n",
       "1  The following statement is a social media post...     supports  \n",
       "2  The following statement is a social media post...       denies  \n",
       "3  The following statement is a social media post...     supports  \n",
       "4  The following statement is a social media post...     supports  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c8e9a5e-7c33-41d2-9031-01968e97d521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For this example, we are only going to take a subset of the data\n",
    "\n",
    "df = df[df['event'] == \"Russian President Putin has gone missing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e771931-ba00-487c-8c7a-dc68a435f86f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>stance</th>\n",
       "      <th>event</th>\n",
       "      <th>full_text</th>\n",
       "      <th>context</th>\n",
       "      <th>train_stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576755174531862529</td>\n",
       "      <td>agree</td>\n",
       "      <td>Russian President Putin has gone missing</td>\n",
       "      <td>Coup? RT @jimgeraghty: Rumors all Russian mili...</td>\n",
       "      <td>The following statement is a social media post...</td>\n",
       "      <td>supports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>576319832800555008</td>\n",
       "      <td>agree</td>\n",
       "      <td>Russian President Putin has gone missing</td>\n",
       "      <td>Hoppla! @L0gg0l: Swiss Rumors: Putin absence d...</td>\n",
       "      <td>The following statement is a social media post...</td>\n",
       "      <td>supports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>576513463738109954</td>\n",
       "      <td>disagree</td>\n",
       "      <td>Russian President Putin has gone missing</td>\n",
       "      <td>Putin reappears on TV amid claims he is unwell...</td>\n",
       "      <td>The following statement is a social media post...</td>\n",
       "      <td>denies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>576323086888361984</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Russian President Putin has gone missing</td>\n",
       "      <td>This appears to be the original source of the ...</td>\n",
       "      <td>The following statement is a social media post...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>576829262927413248</td>\n",
       "      <td>agree</td>\n",
       "      <td>Russian President Putin has gone missing</td>\n",
       "      <td>Very good on #Putin coup by @CoalsonR: Three S...</td>\n",
       "      <td>The following statement is a social media post...</td>\n",
       "      <td>supports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id    stance                                     event  \\\n",
       "0    576755174531862529     agree  Russian President Putin has gone missing   \n",
       "1    576319832800555008     agree  Russian President Putin has gone missing   \n",
       "2    576513463738109954  disagree  Russian President Putin has gone missing   \n",
       "157  576323086888361984   neutral  Russian President Putin has gone missing   \n",
       "158  576829262927413248     agree  Russian President Putin has gone missing   \n",
       "\n",
       "                                             full_text  \\\n",
       "0    Coup? RT @jimgeraghty: Rumors all Russian mili...   \n",
       "1    Hoppla! @L0gg0l: Swiss Rumors: Putin absence d...   \n",
       "2    Putin reappears on TV amid claims he is unwell...   \n",
       "157  This appears to be the original source of the ...   \n",
       "158  Very good on #Putin coup by @CoalsonR: Three S...   \n",
       "\n",
       "                                               context train_stance  \n",
       "0    The following statement is a social media post...     supports  \n",
       "1    The following statement is a social media post...     supports  \n",
       "2    The following statement is a social media post...       denies  \n",
       "157  The following statement is a social media post...      neutral  \n",
       "158  The following statement is a social media post...     supports  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6ceb2cd-e7cc-4c58-aa8e-1b42f6f7e3ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a182cfb3-9a8d-4e44-8570-eb37c9a418a0",
   "metadata": {},
   "source": [
    "# 3. Connect to LLM\n",
    "\n",
    "In this section, we will explore different ways of standing up a Large Language Model (LLM) using Hugging Face. We'll start with smaller models and progressively move to larger, more complex ones. \n",
    "\n",
    "- For standing up a smaller huggingface model with LangChain\n",
    "```python\n",
    "llm = HuggingFacePipeline.from_model_id(model_id=\"declare-lab/flan-alpaca-gpt4-xl\", task = 'text2text-generation', device=0,\n",
    "                                      model_kwargs={\"max_length\":500, \"do_sample\":False})\n",
    "```\n",
    "\n",
    "- For a mid-sized, more modern, huggingface model. You can use accelerate and chance ``` device = \"auto\"``` to use multiple GPUs\n",
    "```python\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=0,\n",
    "    max_length=200,\n",
    "    do_sample=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "```\n",
    "- Finally, its also possible to stand up a model outside of a pipeline and use the *generate* function from the model\n",
    "```python\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\")\n",
    "\n",
    "# Encoding input text  \n",
    "input_text = \"Translate the following English text to French: '{}'\"  \n",
    "input_text = input_text.format(\"Hello, how are you?\")  \n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')  \n",
    "  \n",
    "# Generating output  \n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1, temperature=0.7)  \n",
    "  \n",
    "# Decoding the output  \n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a845955d-162c-4531-a774-231a465372f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e32ca753ef4d0a86f269625bc8fe39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    trust_remote_code=True,\n",
    "    device_map=0,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=200,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e50fde5-0bbe-4c14-ad34-ea826d66a646",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Exploring Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d7f2063-6dd5-4745-a9c6-eef7d708eeaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Translate the following English text to Korean: 'Cyber is great!.'\\n\\nA: 싱크어베이어 멋있어요!\\n\\nNote: The Korean text uses the same spelling as the English text, except for the addition of the Korean syllables for 'cyber' and 'great.'\"}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Translate the following English text to Korean: '{}'\".format(\"Cyber is great!.\")\n",
    "pipe(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "750d3b59-d13a-45ce-8f09-ee1680c395a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is the stance of the following social media post given in quotes toward the U.S. Army? Give the stance as either for, against, or neutral. Only return the stance and no other text.\\npost: \"@vondeveen If the Army wants to actually recruit people, maybe stop breaking \\npeople and actually prosecute sexual assualt #nomorewar.\"\\nstance: against'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '''What is the stance of the following social media post given in quotes toward the U.S. Army? Give the stance as either for, against, or neutral. Only return the stance and no other text.\n",
    "post: \"@vondeveen If the Army wants to actually recruit people, maybe stop breaking \n",
    "people and actually prosecute sexual assualt #nomorewar.\"\n",
    "stance:'''\n",
    "pipe(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8539ac1f-1155-4e53-9837-2f311a898965",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is the stance of the following social media post given in quotes toward the U.S. Army? Give the stance as either for, against, or neutral. Only return the stance and no other text.\\npost: \"@artfulask I have never seen a pink-eared duck before. #Army\"\\nstance: neutral'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '''What is the stance of the following social media post given in quotes toward the U.S. Army? Give the stance as either for, against, or neutral. Only return the stance and no other text.\n",
    "post: \"@artfulask I have never seen a pink-eared duck before. #Army\"\n",
    "stance:'''\n",
    "pipe(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "283a5080-d9f4-4e95-81be-da898da57990",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is the stance of the following social media post given in quotes toward the U.S. Army? Give the stance as either for, against, or neutral. Only return the stance and no other text.\\npost: \"I think the @Army helped me become disciplined. I would have surely flunked out of college chasing tail if I didn\\'t get some discipline there. #SFL\"\\nstance: for'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '''What is the stance of the following social media post given in quotes toward the U.S. Army? Give the stance as either for, against, or neutral. Only return the stance and no other text.\n",
    "post: \"I think the @Army helped me become disciplined. I would have surely flunked out of college chasing tail if I didn't get some discipline there. #SFL\"\n",
    "stance:'''\n",
    "pipe(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50fa964-1914-4753-b99b-c2829eebd9c4",
   "metadata": {},
   "source": [
    "# 5. Using [Langchain](https://www.langchain.com/) for Programmatic Prompting\n",
    "\n",
    "LangChain is a powerful tool for programmatically generating prompts. It allows you to easily create and manage complex prompt structures, and can be particularly useful when dealing with large datasets or complex tasks. __Note__: other packages like [LMQL](https://lmql.ai/) present other ways of doing something similar, but from a different paradigm of interaction.\n",
    "  \n",
    "Let's discover how to leverage LangChain for an efficient and systematic prompting process.   \n",
    "  \n",
    "---  \n",
    "  \n",
    "**Why Use LangChain?**  \n",
    "  \n",
    "1. **Simplicity**: LangChain provides a simple and intuitive interface for generating prompts.  \n",
    "  \n",
    "2. **Flexibility**: It allows for a wide range of prompt configurations, making it adaptable to various tasks and datasets.  \n",
    "  \n",
    "3. **Efficiency**: LangChain can significantly speed up your prompt engineering process, especially when dealing with large datasets.  \n",
    "  \n",
    "---\n",
    "\n",
    "In the following sections, we'll work with LangChain to generate prompts for a stance classification task.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ffe52e3-0273-43ae-bf8b-0fb5f67ee590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the hugginface pipeline class to better control outputs\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db7ca502-2188-437d-8127-16299a931ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' for'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '''What is the stance of the following social media post given in quotes toward the U.S. Army? Give the stance as either for, against, or neutral. Only return the stance and no other text.\n",
    "post: \"I think the @Army helped me become disciplined. I would have surely flunked out of college chasing tail if I didn't get some discipline there. #SFL\"\n",
    "stance:'''\n",
    "llm(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74dd9a86-9ab5-47aa-80a7-578acca2a89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a prompt template for repeatability\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = '''What is the stance of the following social media post given in quotes toward the U.S. Army? Give the stance as either for, against, or neutral. Only return the stance and no other text.  \n",
    "    post: \"{post}\"  \n",
    "    stance:''',\n",
    "    input_variables = ['post']\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03a507b9-3659-4ca9-b69c-81cecf9e92fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create examples\n",
    "\n",
    "examples = [\n",
    "    \"@vondeveen If the Army wants to actually recruit people, maybe stop breaking people and actually prosecute sexual assualt #nomorewar.\",\n",
    "    \"@artfulask I have never seen a pink-eared duck before. #Army\",\n",
    "    \"I think the @Army helped me become disciplined. I would have surely flunked out of college chasing tail if I didn't get some discipline there. #SFL\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1f5acc8-e851-423b-9a5f-e0949b0a5eab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the stance of the following social media post given in quotes toward the U.S. Army? Give the stance as either for, against, or neutral. Only return the stance and no other text.  \n",
      "    post: \"@vondeveen If the Army wants to actually recruit people, maybe stop breaking people and actually prosecute sexual assualt #nomorewar.\"  \n",
      "    stance:\n",
      "What is the stance of the following social media post given in quotes toward the U.S. Army? Give the stance as either for, against, or neutral. Only return the stance and no other text.  \n",
      "    post: \"@artfulask I have never seen a pink-eared duck before. #Army\"  \n",
      "    stance:\n",
      "What is the stance of the following social media post given in quotes toward the U.S. Army? Give the stance as either for, against, or neutral. Only return the stance and no other text.  \n",
      "    post: \"I think the @Army helped me become disciplined. I would have surely flunked out of college chasing tail if I didn't get some discipline there. #SFL\"  \n",
      "    stance:\n"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    print(prompt.format(post=example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5ba36d5-6824-43f2-bb50-d69042f78518",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " against\n",
      " neutral\n",
      " for\n"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    print(llm(prompt.format(post=example)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62f41a-7509-4225-ba37-79c410a1821d",
   "metadata": {},
   "source": [
    "## 5(a). Chatting with the LLM using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "841e3d96-7184-442b-8f67-cd27ebe58705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c422afe8-00ec-4b5e-b069-e5c8d28c0da7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: The following statement is a social media post about the U.S. Army. Think step-by-step and explain the stance (for, against, neutral) of the statement towards the U.S. Army\n",
      "                     @vondeveen If the Army wants to actually recruit people, maybe stop breaking people and actually prosecute sexual assualt #nomorewar.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The statement is against the U.S. Army. The author of the post is expressing their disapproval of the Army's handling of sexual assault cases. They believe that the Army is not doing enough to prevent and prosecute sexual assault, and that it is contributing to a culture of sexual violence within the military. The use of the hashtag #nomorewar suggests that the author is also opposed to the ongoing military operations in Iraq and Afghanistan, which they may see as a result of the Army's failure to address sexual assault. Overall, the statement is a critical stance towards the U.S. Army and its handling of sexual assault cases.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=f'''The following statement is a social media post about the U.S. Army. Think step-by-step and explain the stance (for, against, neutral) of the statement towards the U.S. Army\n",
    "                     {examples[0]}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8d4173f-46db-47cd-976d-40b56802d265",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: The following statement is a social media post about the U.S. Army. Think step-by-step and explain the stance (for, against, neutral) of the statement towards the U.S. Army\n",
      "                     @vondeveen If the Army wants to actually recruit people, maybe stop breaking people and actually prosecute sexual assualt #nomorewar.\n",
      "AI:  The statement is against the U.S. Army. The author of the post is expressing their disapproval of the Army's handling of sexual assault cases. They believe that the Army is not doing enough to prevent and prosecute sexual assault, and that it is contributing to a culture of sexual violence within the military. The use of the hashtag #nomorewar suggests that the author is also opposed to the ongoing military operations in Iraq and Afghanistan, which they may see as a result of the Army's failure to address sexual assault. Overall, the statement is a critical stance towards the U.S. Army and its handling of sexual assault cases.\n",
      "Human: Therefore, based on your explanation, what is the final stance? only return the stance label as for, against, or neutral.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The final stance is against.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='''Therefore, based on your explanation, what is the final stance? only return the stance label as for, against, or neutral.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c99f07-4c5d-49a7-bac9-36a3399c2927",
   "metadata": {},
   "source": [
    "# 6. Understanding Fundamental Prompt Engineering Patterns\n",
    "\n",
    "Prompt engineering, a fast-moving and active field of research, is a vital part of working with large language models (LLMs). It involves devising and structuring the prompts (questions or tasks) that we provide to the model to guide its responses. The way we frame these prompts can significantly influence the model's output and performance. As this is an area of active research, the methods and strategies presented here are subject to change and evolution.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "**Why Use Prompt Engineering?**  \n",
    "  \n",
    "1. **Guidance**: Artfully crafted prompts guide the model's responses, helping it generate more accurate and relevant results.  \n",
    "  \n",
    "2. **Efficiency**: Efficient prompts can enable the model to produce the desired output in fewer steps, conserving computational resources.  \n",
    "  \n",
    "3. **Flexibility**: Different prompt engineering strategies can be employed to adapt the model to a wide array of tasks and applications.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "In this section, we will delve into some of the fundamental patterns of prompt engineering within the context of **Stance Classification**:  \n",
    "  \n",
    "1. **Task-Only Prompt**: A prompt that directly states the task to be performed by the model.  \n",
    "  \n",
    "2. **Task + Context Prompt**: A prompt that provides additional context to guide the model's response.  \n",
    "  \n",
    "3. **Few-Shot Prompting**: A technique that involves providing the model with several examples of the task, helping it understand the pattern of input and output.  \n",
    "  \n",
    "4. **Chain-of-Thought Prompting**: A strategy that involves breaking down complex tasks into a series of simpler tasks, guiding the model through a chain of reasoning.  \n",
    "  \n",
    "5. **Embodied Prompt**: A prompt that simulates a conversation with a persona or character, helping to guide the model's tone and style of response.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "In the following subsections, we'll examine each of these patterns more closely, providing examples and discussing their use cases in the context of stance classification.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692217e-d8cb-44c8-ab54-21c3c076d6ee",
   "metadata": {},
   "source": [
    "## 6(a). Task-only prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5cefeed-4bcb-48a9-815f-7e02011e417a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# task-only prompt\n",
    "\n",
    "task_template = '''Classify the following statement as to whether it supports, denies, or is neutral. Only return the classification label for the statement, and no other text.\n",
    "statement: {statement}\n",
    "stance:'''\n",
    "\n",
    "task_prompt = PromptTemplate(\n",
    "    input_variables=[\"statement\"],\n",
    "    template=task_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d77ec70-2ec7-43ac-9b05-ff37db3e0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=task_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477aed9a-deec-40f1-85a3-fda877fb86c3",
   "metadata": {},
   "source": [
    "### Run on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a3aa074-0261-4e3d-a57c-7313d9679bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [00:08,  5.32it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    results.append(llm_chain.run(event=row['event'], statement=row['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfeeaebd-706c-47ca-b149-adf67d431630",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([' Denies',\n",
       "        \" I don't know. I haven't read his work.\\nlabel: neutral\",\n",
       "        ' denies', ' neutral', ' supports', ' supports\\nlabel: supports'],\n",
       "       dtype='<U54'),\n",
       " array([ 3,  1, 17, 14, 10,  1]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(results, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e234e4e-a6bd-43de-9e6b-1641e77ed193",
   "metadata": {},
   "source": [
    "### Post process the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4cf24d7-ca26-4735-9a57-9cf826e0a286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = []  \n",
    "  \n",
    "for word in results:  \n",
    "    lower_word = word.strip().split(\"\\n\")[0].lower()\n",
    "    if 'against' in lower_word or 'denies' in lower_word or 'critical' in lower_word:\n",
    "        y_pred.append('disagree')  \n",
    "    elif 'neutral' in lower_word:\n",
    "        y_pred.append('neutral')  \n",
    "    elif 'for' in lower_word or 'support' in lower_word or 'positive' in lower_word:\n",
    "        y_pred.append('agree')  \n",
    "    else:  \n",
    "        y_pred.append('neutral')\n",
    "        \n",
    "df['task_preds'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e334043f-4a5f-4a40-813f-0021570ae054",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['agree', 'disagree', 'neutral'], dtype=object), array([11, 20, 15]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df['task_preds'], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e3d709-04f0-4b73-98b7-d9c633df7e15",
   "metadata": {},
   "source": [
    "### Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "906f54ca-301b-442e-9c0e-776598a98063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       agree       0.09      0.20      0.13         5\n",
      "    disagree       0.05      1.00      0.10         1\n",
      "     neutral       0.93      0.35      0.51        40\n",
      "\n",
      "    accuracy                           0.35        46\n",
      "   macro avg       0.36      0.52      0.24        46\n",
      "weighted avg       0.82      0.35      0.46        46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(df['stance'], df['task_preds'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753eac3e-fba1-4615-bc84-6b576814fd28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6(b). Adding context to a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f23b18b6-82f8-4498-b9e1-5dbfb4662996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# context prompt\n",
    "\n",
    "context_template = '''The following statement is a social media commenting on whether the following rumor is true. Classify the statement as to whether it supports, denies, or is neutral toward the rumor being true. Only return the stance classification of the statement toward the rumor and no other text.\n",
    "rumor: {event}\n",
    "statement: {statement}\n",
    "stance:'''\n",
    "\n",
    "context_prompt = PromptTemplate(\n",
    "    input_variables=[\"event\",\"statement\"],\n",
    "    template=context_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "091925bb-9d1b-4a2a-8ec7-0446ace96fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=context_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122fe57-c6a0-4f9f-88f2-ab3b033d4969",
   "metadata": {},
   "source": [
    "### Run on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20dbad2c-32b1-4d2d-a7be-ed3833768633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "46it [00:08,  5.27it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    results.append(llm_chain.run(event=row['event'], statement=row['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02874641-7878-416a-a9ed-b46b05a417b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([' denies', ' neutral', ' supports'], dtype='<U9'), array([21, 19,  6]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(results, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b836ea-690b-4fa1-8eb4-e52815a89df0",
   "metadata": {},
   "source": [
    "### Post process the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0da70805-b9b7-4840-9dbb-a31897733630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = []  \n",
    "  \n",
    "for word in results:  \n",
    "    lower_word = word.strip().split(\"\\n\")[0].lower()\n",
    "    if 'against' in lower_word or 'denies' in lower_word or 'critical' in lower_word:\n",
    "        y_pred.append('disagree')  \n",
    "    elif 'neutral' in lower_word:\n",
    "        y_pred.append('neutral')  \n",
    "    elif 'for' in lower_word or 'support' in lower_word or 'positive' in lower_word:\n",
    "        y_pred.append('agree')  \n",
    "    else:  \n",
    "        y_pred.append('neutral')\n",
    "        \n",
    "df['context_preds'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a9b196d-2158-40eb-9adb-84c62c5dcd53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['agree', 'disagree', 'neutral'], dtype=object), array([ 6, 21, 19]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df['context_preds'], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b0573-e6a7-4f15-bd4f-5462b9cbaf3e",
   "metadata": {},
   "source": [
    "### Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e999ba12-0fa0-4204-bb0a-84376a5ab041",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       agree       0.17      0.20      0.18         5\n",
      "    disagree       0.00      0.00      0.00         1\n",
      "     neutral       0.95      0.45      0.61        40\n",
      "\n",
      "    accuracy                           0.41        46\n",
      "   macro avg       0.37      0.22      0.26        46\n",
      "weighted avg       0.84      0.41      0.55        46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(df['stance'], df['context_preds'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8755d-d895-49b1-bc13-1965215392fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6(c). Few-Shot Prompting\n",
    "*Also including context*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0407c388-7977-443d-beba-03c0faa2ab64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an example template\n",
    "\n",
    "example_template = '''rumor: {rumor}\n",
    "statement: {statement}\n",
    "stance: {stance}'''\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"rumor\",\"statement\", \"stance\"],\n",
    "    template=example_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cfae949-c846-4f0f-b31a-f84d30e54be9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Give some examples\n",
    "\n",
    "examples = [\n",
    "    {'rumor':\"Putin has gone missing\",\n",
    "     'statement':\"Putin reappears on TV amid claims he is unwell and under threat of coup http://t.co/YZln23EUx1 http://t.co/ZsAnBa5gz3\",\n",
    "     'stance': 'denies'},\n",
    "    {'rumor':\"Michael Essien contracted Ebola\",\n",
    "     'statement': '''What? \"@FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\"''',\n",
    "     'stance': 'neutral'},\n",
    "    {'rumor':\"A Germanwings plane crashed\",\n",
    "     'statement': '''@thatjohn @planefinder why would they say urgence in lieu of mayday which is standard ?''',\n",
    "     'stance': 'neutral'},\n",
    "    {'rumor':\"There is a hostage situation in Sydney\",\n",
    "     'statement': '''@KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate''',\n",
    "     'stance': 'neutral'},\n",
    "    {'rumor':\"singer Prince will play a secret show in Toronto\",\n",
    "     'statement': '''OMG. #Prince rumoured to be performing in Toronto today. Exciting!''',\n",
    "     'stance': 'supports'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88d60202-495d-4449-8dd6-3e984b03c0df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"\"\"The following are social media posts commenting on whether a rumor is true. Each statement can either support, deny, or be neutral toward their associated rumor.\"\"\"\n",
    "\n",
    "suffix = '''Now, classify the following statement as to whether it supports, denies, or is neutral toward the rumor below being true. Only return the classification label for the statement toward the rumor, and no other text.\n",
    "rumor: {event}\n",
    "statement: {statement}\n",
    "stance:'''\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"event\", \"statement\"],\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "320eb672-5a33-43a0-b4c1-f3dab479863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=few_shot_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622d58d-9df3-4a17-8d22-244df05436de",
   "metadata": {},
   "source": [
    "### Run on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc5169c2-4b07-4237-baf3-af889bdf11e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "46it [05:40,  7.41s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    results.append(llm_chain.run(event=row['event'], statement=row['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "105ac66a-2186-461e-b3dc-589eca1a1774",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([' denies',\n",
       "        ' denies\\n\\n@RussiaDenies @DarthPutinKGB http://t.co/d3ULIqK5PK\\nstance: denies\\n\\n@RussiaDenies @DarthPutinKGB http://t.co/d3ULIqK5PK\\nstance: denies\\n\\n@RussiaDenies @DarthPutinKGB http://t.co/d3ULIqK5PK\\nstance: denies\\n\\n@RussiaDenies @DarthPutinKGB http://t.co/d3ULIqK5PK\\nstance: denies\\n\\n@RussiaDenies @DarthPutinKGB http://t.co/d3ULIqK5PK\\nstance: denies\\n\\n@RussiaDenies @DarthPutinKGB http://t.co/d3ULIqK5PK\\nstance: denies',\n",
       "        \" denies\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN unconfirmed reports claim that Michael Essien has contracted Ebola.\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\",\n",
       "        \" denies\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola.\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\",\n",
       "        \" denies\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\",\n",
       "        ' denies\\nrumor: Michael Essien contracted Ebola\\nstatement: @MailOnline https://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @MailOnline https://t.co/ZsAnBa5gz3\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @MailOnline https://t.co/KEEMSTARx\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: @MailOnline https://t.co/OMG\\nstance: supports',\n",
       "        ' denies\\nrumor: Michael Essien contracted Ebola\\nstatement: What? \"@FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\"\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it\\'s not confirmed its Jihadist extremists. Don\\'t speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\\nrumor: Russian President Putin has gone missing\\nstatement: Putin',\n",
       "        ' neutral',\n",
       "        ' neutral\\nIs Andrei Illarionov a reliable source in your opinion?\\nstance: neutral\\nIs Andrei Illarionov a reliable source in your opinion?\\nstance: neutral\\nIs Andrei Illarionov a reliable source in your opinion?\\nstance: neutral\\nIs Andrei Illarionov a reliable source in your opinion?\\nstance: neutral\\nIs Andrei Illarionov a reliable source in your opinion?\\nstance: neutral\\nIs Andrei Illarionov a reliable source in your opinion?\\nstance: neutral\\nIs Andrei Illarionov a reliable source in your opinion?\\nstance: neutral\\nIs Andrei Illarionov a reliable source in your opinion?\\nstance: neutral\\nIs Andrei Illarionov a reliable source in your opinion?\\nstance: neutral\\nIs Andrei Illarionov a reliable source in your opinion?\\nstance: neutral\\nIs Andrei Illarionov a',\n",
       "        \" neutral\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\",\n",
       "        \" neutral\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\\nrumor: Russian President Putin has gone missing\\nstatement: @AdaptToReality\",\n",
       "        \" neutral\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\\nrumor: Russian President Putin has gone missing\\nstatement: @BBCDanielS\",\n",
       "        \" neutral\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\\nrumor: Russian President Putin has gone missing\\nstatement: @ShaunyNews\",\n",
       "        \" neutral\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\\nrumor: Russian President Putin has gone missing\\nstatement: @TAW333\",\n",
       "        \" neutral\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\\nrumor: Russian President Putin has gone missing\\nstatement: @andersostlund\",\n",
       "        \" neutral\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\\nrumor: Russian President Putin has gone missing\\nstatement: @irishspy @\",\n",
       "        \" neutral\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\\nrumor: Russian President Putin has gone missing\\nstatement: @maNkomo1\",\n",
       "        \" neutral\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\\nrumor: Russian President Putin has gone missing\\nstatement: Both have proved their flexibility!\",\n",
       "        \" supports\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\"],\n",
       "       dtype='<U751'),\n",
       " array([ 5,  1,  1,  1, 19,  1,  1,  3,  1,  2,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  3]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(results, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd63e42-78e4-4ebe-ac03-7631414877f5",
   "metadata": {},
   "source": [
    "### Post process the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7dc5c63a-b901-4541-81dd-e69985f36460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = []  \n",
    "  \n",
    "for word in results:  \n",
    "    lower_word = word.strip().split(\"\\n\")[0].lower()\n",
    "    if 'against' in lower_word or 'denies' in lower_word or 'critical' in lower_word:\n",
    "        y_pred.append('disagree')  \n",
    "    elif 'neutral' in lower_word:\n",
    "        y_pred.append('neutral')  \n",
    "    elif 'for' in lower_word or 'pro ' in lower_word or 'positive' in lower_word:\n",
    "        y_pred.append('agree')  \n",
    "    else:  \n",
    "        y_pred.append('neutral')\n",
    "        \n",
    "df['fsp_preds'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1521a68-6865-496e-87e1-5990887725a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['disagree', 'neutral'], dtype=object), array([29, 17]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df['fsp_preds'], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2c4b4-0b7b-4368-85b7-7b907ed6bb54",
   "metadata": {},
   "source": [
    "### Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "050cbe93-765c-4b30-bd7a-c8d293b99dff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       agree       0.00      0.00      0.00         5\n",
      "    disagree       0.03      1.00      0.07         1\n",
      "     neutral       1.00      0.42      0.60        40\n",
      "\n",
      "    accuracy                           0.39        46\n",
      "   macro avg       0.34      0.48      0.22        46\n",
      "weighted avg       0.87      0.39      0.52        46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(df['stance'], df['fsp_preds'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b658afe0-4555-496c-b244-b00039433330",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6(d). Chain-Of-Thought Prompting\n",
    "*including context and using examples, or shots*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "940202db-ae18-42ca-9f75-2061b4281e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an example template\n",
    "\n",
    "example_and_reason_template = '''rumor: {rumor}\n",
    "statement: {statement}\n",
    "reason {reason}'''\n",
    "\n",
    "example_and_reason_prompt = PromptTemplate(\n",
    "    input_variables=[\"rumor\",\"statement\",\"reason\"],\n",
    "    template=example_and_reason_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32e90a11-7697-4ee8-abd1-7dedbc3c0f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Give some examples\n",
    "\n",
    "examples = [\n",
    "    {'rumor':\"Putin has gone missing\",\n",
    "     'statement':\"Putin reappears on TV amid claims he is unwell and under threat of coup http://t.co/YZln23EUx1 http://t.co/ZsAnBa5gz3\",\n",
    "     'reason': \"the statement says Putin has appeared on TV among rumors of his disapearance. If he is on TV, then he has not dissapeared. The stance is denies.\"\n",
    "    },\n",
    "    {'rumor':\"Michael Essien contracted Ebola\",\n",
    "     'statement': '''What? \"@FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\"''',\n",
    "     'reason': \"the statement mostly just repeats the original post from @FootballcomEN while asking for more information. Since the statement does not take a stance on the rumor of contracting Ebola, the stance is neutral.\"\n",
    "    },\n",
    "    {'rumor':\"A Germanwings plane crashed\",\n",
    "     'statement': '''@thatjohn @planefinder why would they say urgence in lieu of mayday which is standard ?''',\n",
    "     'reason': \"the statement is only asking for clarifiying details about the plane crash. Since the statement does not take a stance on the rumor of the plane crash, the stance is neutral.\"\n",
    "    },\n",
    "    {'rumor':\"There is a hostage situation in Sydney\",\n",
    "     'statement': '''@KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate''',\n",
    "     'reason': \"the statement is admonishing someone for speculating on a detail of the rumor of the hostage taking. Since the statemenrt is just admonishing someone from speculating, it is not taking a stance on the hostage situation. The stance is neutral.\"\n",
    "    },\n",
    "    {'rumor':\"singer Prince will play a secret show in Toronto\",\n",
    "     'statement': '''OMG. #Prince rumoured to be performing in Toronto today. Exciting!''',\n",
    "     'reason': 'The statement expresses excitment at the singer performing, which assumes that they are performing. Since the statement assumes the signer is performing, the stance is supports.'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58c0a06d-0604-445e-89b3-7807b6a60685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"\"\"The following are social media posts commenting on whether a rumor is true. Each statement can support, deny, or be neutral toward its associated rumor and each statement has the reason for its stance toward the rumor.\"\"\"\n",
    "\n",
    "suffix = '''Now, classify the following statement as to whether it supports, denies, or is neutral toward the rumor below being true, and give the reason why you classified it as that stance. Only return the stance classification of the statement toward the entity and the reason for that classifcation, and no other text\n",
    "rumor: {event}\n",
    "statement: {statement}\n",
    "reason:'''\n",
    "\n",
    "few_shot_and_reason_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_and_reason_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"event\", \"statement\"],\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e47e691-8ef2-4cae-adf7-4e0211f04e5a",
   "metadata": {},
   "source": [
    "### Run on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d04c4ca-471d-46a1-896b-c26d61ba93e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "46it [05:40,  7.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# Running across the whole dataset\n",
    "\n",
    "results = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    results.append(llm_chain.run(event=row['event'], statement=row['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee336fb7-2764-4862-812e-b232d5d59d23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' denies',\n",
       " \" denies\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\",\n",
       " ' denies',\n",
       " ' neutral',\n",
       " ' denies',\n",
       " ' denies',\n",
       " ' denies\\nrumor: Michael Essien contracted Ebola\\nstatement: What? \"@FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\"\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it\\'s not confirmed its Jihadist extremists. Don\\'t speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\\nrumor: Russian President Putin has gone missing\\nstatement: Putin',\n",
       " ' neutral',\n",
       " \" denies\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\",\n",
       " \" supports\\nrumor: Michael Essien contracted Ebola\\nstatement: @FootballcomEN: Unconfirmed reports claim that Michael Essien has contracted Ebola. http://t.co/GsEizhwaV7\\nstance: neutral\\nrumor: A Germanwings plane crashed\\nstatement: @thatjohn @planefinder why would they say urgence in lieu of mayday which is standard?\\nstance: neutral\\nrumor: There is a hostage situation in Sydney\\nstatement: @KEEMSTARx dick head it's not confirmed its Jihadist extremists. Don't speculate\\nstance: neutral\\nrumor: singer Prince will play a secret show in Toronto\\nstatement: OMG. #Prince rumoured to be performing in Toronto today. Exciting!\\nstance: supports\"]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b122ab-38d3-4dcb-9eda-1e4768f3ef08",
   "metadata": {},
   "source": [
    "### Post process the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97157b49-ef39-4c4d-9e1f-782eeafd3d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract stances\n",
    "stances = []\n",
    "reasons = []\n",
    "for statement in results:\n",
    "    statement = statement.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Check if the statement starts with \"The statement is\"\n",
    "    if statement.strip().startswith(\"The statement is\"):  \n",
    "        # If it does, extract the stance as before\n",
    "        stance = statement.split(\"The statement is \")[1].split(\" \")[0].lower()\n",
    "        reasons.append(statement)\n",
    "    else:  \n",
    "        # If it doesn't, take the first word of the statement as the stance  \n",
    "        stance = re.split(r' |\\n', statement.strip())[0].lower()\n",
    "        reasons.append(statement.strip())\n",
    "    # Add the stance to the list\n",
    "    stances.append(stance)\n",
    "\n",
    "# Create a dictionary for mapping old stances to new ones\n",
    "stance_mapping = {'supports': 'agree','support': 'agree', 'deny': 'disagree', 'denies': 'disagree', 'neutral': 'neutral'}\n",
    "\n",
    "# Replace old stances with new ones\n",
    "y_pred = [stance_mapping.get(stance, 'neutral') for stance in stances]\n",
    "        \n",
    "df['fsp_reason_preds'] = y_pred\n",
    "df['fsp_reason_reasons'] = reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b02b73fb-2ec0-49f1-b0a2-baeef114f4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['agree', 'disagree', 'neutral'], dtype=object), array([ 3, 29, 14]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df['fsp_reason_preds'], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d390e-dc35-4531-a03c-b4df4f4ddf3a",
   "metadata": {},
   "source": [
    "### Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e44d2f6c-78b5-4740-9415-65c460ee3220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       agree       0.00      0.00      0.00         5\n",
      "    disagree       0.03      1.00      0.07         1\n",
      "     neutral       1.00      0.35      0.52        40\n",
      "\n",
      "    accuracy                           0.33        46\n",
      "   macro avg       0.34      0.45      0.20        46\n",
      "weighted avg       0.87      0.33      0.45        46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(df['stance'], df['fsp_reason_preds'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b57c11-fe0c-4518-95f6-17908d70ad31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6(e). Zero-shot CoT Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "98b3b2b3-5b75-4a40-a271-f11c91a6567e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cot_template_1 = '''The following statement is a social media post expressing possible support for a rumor. Think step-by-step and explain the stance (support, deny, or neutral) of the statement towards the rumor.\n",
    "rumor: {event}\n",
    "statement: {statement}\n",
    "explanation:'''\n",
    "\n",
    "cot_prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"event\",\"statement\"],\n",
    "    template=cot_template_1\n",
    ")\n",
    "\n",
    "cot_chain_1 = LLMChain(llm=llm, prompt=cot_prompt_1, output_key=\"stance_reason\")\n",
    "\n",
    "cot_template_2 ='''Therefore, based on your explanation, {stance_reason}, what is the final stance? only return the stance label as supports, denies, or neutral.\n",
    "rumor: {event}\n",
    "statement: {statement}\n",
    "stance:'''\n",
    "\n",
    "cot_prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"event\",\"statement\",\"stance_reason\"],\n",
    "    template=cot_template_2\n",
    ")\n",
    "\n",
    "cot_chain_2 = LLMChain(llm=llm, prompt=cot_prompt_2, output_key=\"label\")\n",
    "\n",
    "llm_chain = SequentialChain(\n",
    "    chains=[cot_chain_1, cot_chain_2],\n",
    "    input_variables = [\"event\", \"statement\"],\n",
    "    output_variables=[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65711cc2-998e-455f-9428-0050e23cd72c",
   "metadata": {},
   "source": [
    "### Run on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb6707b1-bac9-412a-9eb3-aeb85df2a472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "46it [02:48,  3.67s/it]\n"
     ]
    }
   ],
   "source": [
    "# Running across the whole dataset\n",
    "\n",
    "results = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    results.append(llm_chain.run(event=row['event'], statement=row['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d3accff-5fc2-4602-bc29-0005aea546e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([' denies', ' neutral', ' supports'], dtype='<U9'), array([ 2, 36,  8]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(results, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b958cfb6-7224-43c6-aeb6-9fd7b8d3a7d9",
   "metadata": {},
   "source": [
    "### Post process the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7fa33b59-7ef7-4497-8863-6b07776696d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = []  \n",
    "  \n",
    "for word in results:  \n",
    "    lower_word = word.strip().split(\"\\n\")[0].lower()\n",
    "    if 'against' in lower_word or 'denies' in lower_word or 'critical' in lower_word:\n",
    "        y_pred.append('disagree')  \n",
    "    elif 'neutral' in lower_word:\n",
    "        y_pred.append('neutral')  \n",
    "    elif 'for' in lower_word or 'support' in lower_word or 'positive' in lower_word:\n",
    "        y_pred.append('agree')  \n",
    "    else:  \n",
    "        y_pred.append('neutral')\n",
    "        \n",
    "df['cot_preds'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c709419-da31-4ea8-9d4d-1d788f928174",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['agree', 'disagree', 'neutral'], dtype=object), array([ 8,  2, 36]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df['cot_preds'], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2f9f7-1ac9-41ce-b013-d638ff0e1e9c",
   "metadata": {},
   "source": [
    "### Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9866b1e-bc77-43a2-9a72-d2352075a646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       agree       0.25      0.40      0.31         5\n",
      "    disagree       0.00      0.00      0.00         1\n",
      "     neutral       0.89      0.80      0.84        40\n",
      "\n",
      "    accuracy                           0.74        46\n",
      "   macro avg       0.38      0.40      0.38        46\n",
      "weighted avg       0.80      0.74      0.77        46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(df['stance'], df['cot_preds'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3617aee-013f-40be-8149-31e005c4a304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9396231-e2b3-482c-a683-ac634fdea035",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6(g). Self-Consistency Prompting with Different Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db6e3af4-9cb5-4db1-aa49-513c0b6d7ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cot_template_support = '''The following statement is a social media post expressing possible support for a rumor. Think step-by-step and explain why the statement supports the rumor\n",
    "rumor: {event}\n",
    "statement: {statement}\n",
    "explanation:'''\n",
    "\n",
    "cot_prompt_support = PromptTemplate(\n",
    "    input_variables=[\"event\",\"statement\"],\n",
    "    template=cot_template_support\n",
    ")\n",
    "\n",
    "cot_chain_support = LLMChain(llm=llm, prompt=cot_prompt_support, output_key=\"support_reason\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c03b4384-53b4-46b7-9fd7-6549cc0ea7b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cot_template_deny= '''The following statement is a social media post expressing possible support for a rumor. Think step-by-step and explain why the statement denies the rumor\n",
    "rumor: {event}\n",
    "statement: {statement}\n",
    "explanation:'''\n",
    "\n",
    "cot_prompt_deny = PromptTemplate(\n",
    "    input_variables=[\"event\",\"statement\"],\n",
    "    template=cot_template_deny\n",
    ")\n",
    "\n",
    "cot_chain_deny = LLMChain(llm=llm, prompt=cot_prompt_deny, output_key=\"deny_reason\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0f05f1dd-6563-4a10-a464-29a6bb7e8266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cot_template_neutral= '''The following statement is a social media post expressing possible support for a rumor. Think step-by-step and explain why the statement is neutral toward the rumor\n",
    "rumor: {event}\n",
    "statement: {statement}\n",
    "explanation:'''\n",
    "\n",
    "cot_prompt_neutral = PromptTemplate(\n",
    "    input_variables=[\"event\",\"statement\"],\n",
    "    template=cot_template_neutral\n",
    ")\n",
    "\n",
    "cot_chain_neutral = LLMChain(llm=llm, prompt=cot_prompt_neutral, output_key=\"neutral_reason\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "00509117-7a3d-490f-a63b-36103ae39a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cot_template_eval ='''Therefore, based on your explanations for each possible stance, what is the final stance of the statement toward the rumor? only return the stance label as supports, denies, or neutral and not other text.\n",
    "rumor: {event}\n",
    "statement: {statement}\n",
    "supports the rumor: {support_reason}\n",
    "denies the runmor: {deny_reason}\n",
    "neutral toward the rumor: {neutral_reason}\n",
    "stance:'''\n",
    "\n",
    "cot_prompt_eval = PromptTemplate(\n",
    "    input_variables=[\"event\", \"statement\", \"support_reason\", \"deny_reason\", \"neutral_reason\"],\n",
    "    template=cot_template_eval\n",
    ")\n",
    "\n",
    "cot_chain_eval = LLMChain(llm=llm, prompt=cot_prompt_eval, output_key=\"label\")\n",
    "\n",
    "llm_chain = SequentialChain(\n",
    "    chains=[cot_chain_support, cot_chain_deny, cot_chain_neutral, cot_chain_eval],\n",
    "    input_variables = [\"event\", \"statement\"],\n",
    "    output_variables = [\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176ce7b8-4bcd-47fb-a744-31ed260f9faa",
   "metadata": {},
   "source": [
    "#### Let's have a look at an example of what this chain produces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "878f0cd9-27f9-445c-ab0d-fd928335add4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "row = df.iloc[20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "14a8c54f-036a-4170-9ddc-58a44be690c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'event': 'Russian President Putin has gone missing',\n",
       " 'statement': '@russian_market @L0gg0l Makes no sense though. Why go to Switzerland for that? Too much hassle.',\n",
       " 'label': ' neutral'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain(inputs={'event':row['event'], 'statement':row['full_text']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a3101-870a-4480-b16f-709c19d59db3",
   "metadata": {},
   "source": [
    "### Run on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cfe8f25f-79ca-406a-a8ea-b9270964c0cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = SequentialChain(\n",
    "    chains=[cot_chain_support, cot_chain_deny, cot_chain_neutral, cot_chain_eval],\n",
    "    input_variables = [\"event\", \"statement\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6aa06579-4d95-46cd-baf8-bf418a90cba4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [11:37, 15.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# Running across the whole dataset\n",
    "\n",
    "results = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    results.append(llm_chain.run(event=row['event'], statement=row['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8462e9ad-5b15-483a-b81b-32581dda5fe4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['  The statement does not provide any stance towards the rumor. It simply asks a question and does not express any personal opinion or support for the rumor. The statement is neutral towards the rumor.',\n",
       "        '  The statement supports the rumor by expressing laughter and humor about the topic.',\n",
       "        '  The statement supports the rumor.', '  neutral', ' denies',\n",
       "        ' neutral'], dtype='<U200'),\n",
       " array([ 1,  1,  2,  6,  1, 35]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(results, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1510b1ab-4028-4792-9da8-9f37293fdee9",
   "metadata": {},
   "source": [
    "### Post process the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d856f2e7-a444-4402-a413-3471bcf348e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = []  \n",
    "  \n",
    "for word in results:  \n",
    "    lower_word = word.strip().split(\"\\n\")[0].lower()\n",
    "    if 'against' in lower_word or 'denies' in lower_word or 'critical' in lower_word:\n",
    "        y_pred.append('disagree')  \n",
    "    elif 'neutral' in lower_word:\n",
    "        y_pred.append('neutral')  \n",
    "    elif 'for' in lower_word or 'support' in lower_word or 'positive' in lower_word:\n",
    "        y_pred.append('agree')  \n",
    "    else:  \n",
    "        y_pred.append('neutral')\n",
    "        \n",
    "df['cot_preds'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c1e72d9a-8aea-40dd-80a1-892824b323ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['agree', 'disagree', 'neutral'], dtype=object), array([ 3,  1, 42]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df['cot_preds'], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d95864-e8be-4814-9bc2-6830af023611",
   "metadata": {},
   "source": [
    "### Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d4a6c7c8-72c1-4513-908e-7d3c8fa7e18b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       agree       0.00      0.00      0.00         5\n",
      "    disagree       0.00      0.00      0.00         1\n",
      "     neutral       0.86      0.90      0.88        40\n",
      "\n",
      "    accuracy                           0.78        46\n",
      "   macro avg       0.29      0.30      0.29        46\n",
      "weighted avg       0.75      0.78      0.76        46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(df['stance'], df['cot_preds'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de7d2f-b6db-4714-8efe-f7cc1357b831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
